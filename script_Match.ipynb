{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports modules and libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from ConvertFile import Convert_File_to_pd\n",
    "from DataAnalyze import DataAnalyze\n",
    "from AnalyzeDuplicatedData import AnalyzeDuplicatedData\n",
    "from DataTransform import DataTransform\n",
    "import re \n",
    "from cmath import isnan\n",
    "from ObsMatchFields import ObsMatchFields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform CSVs to DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_frame=Convert_File_to_pd(file_name='Match',name_table='Data',folder_name='ETL-Data-FIFA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df=Data_frame.convert_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis to investigate the possibility of data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimindo o DataFrame\n",
    "Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como são 115 colunas, é necessário prioriza-las \n",
    "\n",
    "for col in Df.columns:\n",
    "    print(col,end=' , ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para dar prioridade, os campos dos players, assim como as siglas contendo as 3 letras maiusculas serão removidos, pois tem campos acima que retraram \n",
    "#estatisticas de partidas, assim como os ids que irão servir para referenciar outras tabelas \n",
    "\n",
    "#Para o caso dos players, criou-se dois vetores que logicamente terão os mesmos tamanhos, já que o número dos jogadores de casa devem ser os mesmos dos visitantes\n",
    "ls_col=Df.columns.tolist()\n",
    "arra_home_p=list(Df.columns.str.startswith('home_player_'))\n",
    "arra_away_p=list(Df.columns.str.startswith('away_player_'))\n",
    "\n",
    "#Olhando para lista acima, percebe-se que o padrão é de 3 letras maisculas ou 4 com 365\n",
    "#Seria possível de fazer uma reg expression para detectar, mas para ficar mais simples é só identificar o indice do B365H, que depois dele todos serão evitados\n",
    "#Como existem Falses nas condições lógicas abaixo, esse vetor original considerado sera deslocado, logo subtraindo os Trues, teremos o vetor filtrado\n",
    "\n",
    "ls_fin=[ls_col[a] for a in range(len(arra_home_p)) if arra_home_p[a]==False and arra_away_p[a]==False][:ls_col.index('B365H')-sum(arra_home_p+arra_away_p)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dici_updated={}\n",
    "\n",
    "for s in ls_fin:\n",
    "    v=Df[s]\n",
    "    dici_updated[s]=v\n",
    "\n",
    "Df=pd.DataFrame(dici_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe responsável pelas análises exploratórias\n",
    "Results_analysis=DataAnalyze(Df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliando a proporção de NaNs em todos os campos\n",
    "Results_analysis.count_nans(name_columns=list(Df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percebe-se que as ultimas colunas referentes aos acontecimentos da partida tem o mesmo valor da proporção de NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##visando investigar se os dados em não NaN são os mesmos\n",
    "#Como a coluna goal já existe como info em outros campos anteriores, será avaliado para os demais campos\n",
    "#Fora isso, pegaremos somente as 4 últimas colunas para não gerar ainda mais complexidade nas futuras inferências\n",
    "Df.drop(['goal','shoton','shotoff'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "dici_not_nans={}\n",
    "r=list(Df.columns)[11:]\n",
    "for t in r:\n",
    "    temp=Df[t].notna()\n",
    "    q=[Df[t][x] for x in range(temp.shape[0]) if temp[x]]\n",
    "    dici_not_nans[t]=q\n",
    "\n",
    "\n",
    "Df_temp=pd.DataFrame(dici_not_nans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### criação de novas colunas com base nas antigas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COLUNA FOUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para detectar a quantidade de ocorrencias de um time de casa ou time de fora\n",
    "\n",
    "def cont_by_teams(Df,name_of_column,name_key_home_team,name_key_away_team):\n",
    "    ds={}\n",
    "    for z in range((Df.shape[0])):\n",
    "        if isinstance(Df[name_of_column][z],str):\n",
    "            home_team=Df['home_team_api_id'][z]\n",
    "            away_team=Df['away_team_api_id'][z]\n",
    "            \n",
    "      \n",
    "\n",
    "            rgxx_team=\"((<team>)[0-9]+(</team>))\"\n",
    "\n",
    "            string_aval=re.findall(rgxx_team,Df[name_of_column][z])\n",
    "            \n",
    "            \n",
    "            cont_home,cont_away=0,0\n",
    "            for s in string_aval:\n",
    "\n",
    "                if s != None:\n",
    "                    comp1=len(s[0])\n",
    "                    value=s[0][len(\"<team>\"):comp1-len(\"</team>\")]\n",
    "                   \n",
    "                \n",
    "                    if value == str(home_team): cont_home+=1\n",
    "                    else: cont_away+=1\n",
    "                \n",
    "            ds[z]={name_key_home_team:cont_home,name_key_away_team:cont_away}\n",
    "        \n",
    "        else:\n",
    "            ds[z]={name_key_home_team:0,name_key_away_team:0}\n",
    "\n",
    "\n",
    "    return ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percebe-se abaixo que é possível extrair informações posteriores a respeito de tempo, o time em que executou algo e os jogadores\n",
    "#Todavia, para não estabelecer mais complexidade à tabela, os dados serão ramificados em outra(s) colunas, seguindo as estratégias:\n",
    "\n",
    "\n",
    "#Para coluna foulcommit, ramificação em duas colunas da quantidade de faltas cometidas pelo time da casa e pelo time visitante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_foul=cont_by_teams(Df=Df,name_of_column='foulcommit',name_key_home_team=\"quant_home_team_foul_commit\",name_key_away_team=\"quant_away_team_foul_commit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_foul=list(dict_foul.values())\n",
    "home_list,away_list=[],[]\n",
    "\n",
    "for i in lis_foul:\n",
    "    home_list.append(i['quant_home_team_foul_commit'])\n",
    "    away_list.append(i['quant_away_team_foul_commit'])\n",
    "\n",
    "#gerar a lista contendo os valores dos times \n",
    "Df['quant_home_team_foul_commit']=home_list\n",
    "Df['quant_away_team_foul_commit']=away_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropar a coluna antiga\n",
    "Df.drop(['foulcommit'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COLUNA CARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para coluna card, as colunas serão convertidas em contagem de cartões amarelos e vermelhos para os times da casa e de fora\n",
    "\n",
    "ds_card={}\n",
    "\n",
    "for z in range((Df.shape[0])):\n",
    "    if isinstance(Df['card'][z],str):\n",
    "        home_team=Df['home_team_api_id'][z]\n",
    "        away_team=Df['away_team_api_id'][z]\n",
    "        \n",
    "        string_aval=Df['card'][z]\n",
    "\n",
    "        rgxx=\"((<card_type>)[a-z-A-Z]+(<\\/card_type>))\"\n",
    "        rgxx_team=\"((<team>)[0-9]+(<\\/team>))\"\n",
    "\n",
    "        \n",
    "        pat1=re.findall(rgxx_team,string_aval)\n",
    "        pat2=re.findall(rgxx,string_aval)\n",
    "\n",
    "        yhteam,yateam=0,0\n",
    "        rhteam,rateam=0,0\n",
    "        for patt1,patt2 in zip(pat1,pat2):\n",
    "            if patt1!=None:\n",
    "\n",
    "                #Para retirar o id do time no qual realizou a falta para cometer o cartão, o slice é feito a partir do comprimento da primeira tag (<team>)\n",
    "                #na qual contem 6 caracteres, e a partir dele pegar o número(ID do time) até o comprimento - 7(valor de comprimento da tag </team>)\n",
    "                #já que len(<tag>)+len(id_num)+len(</tag>)=len(tot)\n",
    "                comp1=len(patt1[0])\n",
    "                slice_pat1=patt1[0][len(\"<team>\"):comp1-len(\"</team>\")]\n",
    "\n",
    "                #mesma logica para a tag card_type\n",
    "                comp2=len(patt2[0])\n",
    "                slice_pat2=patt2[0][len(\"<card_type>\"):comp2-len(\"</card_type>\")]\n",
    "\n",
    "                if slice_pat2.lower() == 'y':\n",
    "                    if slice_pat1==str(home_team):\n",
    "                        yhteam+=1\n",
    "                    else:yateam+=1\n",
    "\n",
    "                else:\n",
    "                    if slice_pat1==str(home_team):\n",
    "                        rhteam+=1\n",
    "                    else:rateam+=1\n",
    "        \n",
    "        \n",
    "        ds_card[z]={\"number_yellow_cards_home_team\":yhteam,\"number_yellow_cards_away_team\":yateam,\"number_red_cards_home_team\":rhteam,\"number_red_cards_away_team\":rateam}\n",
    "\n",
    "    else:\n",
    "        ds_card[z]={\"number_yellow_cards_home_team\":0,\"number_yellow_cards_away_team\":0,\"number_red_cards_home_team\":0,\"number_red_cards_away_team\":0}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_card=list(ds_card.values())\n",
    "y_home_list,y_away_list,r_home_list,r_away_list=[],[],[],[]\n",
    "\n",
    "for i in lis_card:\n",
    "    y_home_list.append(i[\"number_yellow_cards_home_team\"])\n",
    "    y_away_list.append(i[\"number_yellow_cards_away_team\"])\n",
    "    r_home_list.append(i[\"number_red_cards_home_team\"])\n",
    "    r_away_list.append(i[\"number_red_cards_away_team\"])\n",
    "\n",
    "#gerar a lista contendo os valores dos times \n",
    "Df[\"number_yellow_cards_home_team\"]=y_home_list\n",
    "Df[\"number_yellow_cards_away_team\"]=y_away_list\n",
    "Df[\"number_red_cards_home_team\"]=r_home_list\n",
    "Df[\"number_red_cards_away_team\"]=r_away_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropar coluna antiga\n",
    "Df.drop(['card'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COLUNA CROSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para a coluna cross, quantidade de cruzamentos do time da casa e de fora serão convertidos em colunas \n",
    "\n",
    "ds_cross={}\n",
    "\n",
    "\n",
    "for z in range((Df.shape[0])):\n",
    "    if isinstance(Df['cross'][z],str):\n",
    "        home_team=Df['home_team_api_id'][z]\n",
    "        away_team=Df['away_team_api_id'][z]\n",
    "        \n",
    "\n",
    "        rgxx_team=\"((<team>)[0-9]+(</team>))\"\n",
    "\n",
    "        var_dec=(Df['cross'][z].split(\"</value>\"))\n",
    "        \n",
    "        cont_home_cross,cont_away_cross=0,0\n",
    "        for s in var_dec:\n",
    "            q=re.search(rgx,s)\n",
    "            if q != None:\n",
    "                \n",
    "                q=q.span()\n",
    "                u=(q[0]+len((\"<team>\")),q[1]-len((\"</team>\")))\n",
    "                value=s[u[0]:u[1]]\n",
    "            \n",
    "                if value == str(home_team): cont_home_cross+=1\n",
    "                else: cont_away_cross+=1\n",
    "              \n",
    "        ds_cross[z]={\"quant_home_team_cross\":cont_home_cross,\"quant_away_team_cross\":cont_away_cross}\n",
    "       \n",
    "    else:\n",
    "        ds_cross[z]={\"quant_home_team_cross\":0,\"quant_away_team_cross\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_cross=list(ds_cross.values())\n",
    "cross_home,cross_away=[],[]\n",
    "\n",
    "for i in lis_cross:\n",
    "   cross_home.append(i[\"quant_home_team_cross\"])\n",
    "   cross_away.append(i[\"quant_away_team_cross\"])\n",
    "\n",
    "#gerar a lista contendo os valores dos times \n",
    "\n",
    "Df[\"quant_home_team_cross\"]=cross_home\n",
    "Df[\"quant_away_team_cross\"]=cross_away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropar coluna antiga\n",
    "Df.drop(['cross'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COLUNA CORNER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_corner={}\n",
    "\n",
    "\n",
    "for z in range((Df.shape[0])):\n",
    "    if isinstance(Df['corner'][z],str):\n",
    "        home_team=Df['home_team_api_id'][z]\n",
    "        away_team=Df['away_team_api_id'][z]\n",
    "        \n",
    "\n",
    "        rgxx_team=\"((<team>)[0-9]+(</team>))\"\n",
    "\n",
    "        var_dec=(Df['corner'][z].split(\"</value>\"))\n",
    "        print(var_dec)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Data Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#além disso, é necessário avaliar os registros duplicados quando a coluna não pode contê-los, como é o caso do ID também\n",
    "Result_dup_id=AnalyzeDuplicatedData(dataframe=Df,name_col_data='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliando a quantidade de ids duplicados\n",
    "Result_dup_id.print_duplicated_data(table_name='Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo isso para coluna country_id e name\n",
    "Result_dup_country_id=AnalyzeDuplicatedData(dataframe=Df,name_col_data='country_id')\n",
    "Result_dup_name=AnalyzeDuplicatedData(dataframe=Df,name_col_data='name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_dup_country_id.print_duplicated_data(table_name='Country')\n",
    "Result_dup_name.print_duplicated_data(table_name='Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como a coluna ID e country_id são de identificação, não há valores que possam determinar um outlier, a não ser que eles fossem autoincrementais\n",
    "#Porém, não é o caso, conforme pode ser visto acima no Df \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze  typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando a tipagem esperada de forma geral\n",
    "Df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como no método info a coluna name é object e object pode conter qualquer tipagem, então é necessário investigar se todas são Strings para serem Name\n",
    "Results_analysis.confirm_typing(name_column='name',type_expected='str')\n",
    "\n",
    "#Já os demais devem ser mantidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como não houve acontecimento de NaNs, dados duplicados, outliers e tipagens fora do escopo, o Df já está devidamente tratado\n",
    "Data_Transf=Df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Transf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03807dcc214718de3fef1abbc62da4d562d2ddd650787426faadb17d03c3a4b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
